This project is an intelligent system based on machine learning that can recognize and display various words from sign language using camera images, even in real-time. By leveraging advanced machine learning and computer vision techniques, the system analyzes hand and finger movements and translates them into corresponding words in sign language. The process happens instantaneously, allowing users to see the translation of their gestures displayed on the screen in real-time.

A key feature of this project is the unique dataset, which has been manually and exclusively collected for this purpose. Unlike public datasets that may contain inconsistent or inaccurate samples, this dataset has been carefully selected and created to ensure the highest level of accuracy and efficiency in recognizing sign language gestures. The manual collection means that all gestures were captured with a focus on image quality, viewing angles, and diversity in user movements.

Moreover, the normalization of data has been a crucial part of the system's development. Advanced normalization techniques were applied to standardize the data, enabling the algorithm to more effectively and accurately analyze the differences in hand movements. This step has ensured that the system performs well even under different lighting conditions and angles, reducing recognition errors to a minimum.

Overall, this project stands out due to the integration of advanced machine learning, a custom-built dataset, and cutting-edge normalization methods, making it a powerful tool for assisting the deaf and improving human interactions through sign language. In the future, this system could serve as a platform for the development of other technologies aimed at enhancing human communication and intelligent assistance.
